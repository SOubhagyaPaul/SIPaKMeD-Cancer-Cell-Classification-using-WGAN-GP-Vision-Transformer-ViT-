# SIPaKMeD-Cancer-Cell-Classification-using-WGAN-GP-Vision-Transformer-ViT-
Cancer cell classification using WGAN-GP and Vision Transformer (ViT) on the SIPaKMeD dataset. Includes synthetic image generation, FID evaluation, and ViT training on real, synthetic, and combined datasets. Final metrics: accuracy, F1-score, confusion matrix

Cervical cancer continues to be one of the most significant threats to women’s
health globally, with disproportionate impacts in resource-limited regions where
systematic screening programs are often unavailable. While automated cytology
analysis presents a viable technological approach to bridge this gap, conventional
machine learning systems frequently struggle with the pronounced class imbalance
inherent in medical datasets such as SIPaKMeD.
Our research introduces an innovative framework that synergistically combines
Wasserstein Generative Adversarial Networks (WGANs) with Vision Transformers
(ViTs) to overcome these limitations. The methodology strategically employs
WGANs to produce biologically plausible synthetic cervical cell images, specifically
targeting underrepresented pathological classes to achieve dataset equilibrium.
These augmented samples, when integrated with authentic clinical data,
enable the ViT architecture to develop more discriminative features for reliable
classification.
These synthetic images, combined with real samples, are used to fine-tune a ViT
model for robust classification. Experiments conducted on the SIPaKMeD dataset
show that this augmentation strategy significantly improves classification metrics,
especially in minority classes. Key findings demonstrate that this hybrid methodology
significantly improves classification performance, particularly for minority
classes that are critical for early cancer detection. The ViT model, trained on
the WGAN-augmented dataset, achieves superior performance metrics (accuracy,
precision, recall, and F1-score) compared to baseline approaches.

Research Gap and Direction
Although numerous studies have explored the use of convolutional neural networks
(CNNs) for automated cervical cell classification[1], these approaches often
face limitations related to dataset imbalance, overfitting, and insufficient generalization
to rare but clinically significant cell types. Furthermore, while Generative
Adversarial Networks (GANs) have been successfully applied in various medical
image augmentation tasks, many works use GANs in a general, class-agnostic
manner. This limits their ability to generate meaningful variations tailored to each
class, particularly for minority categories. At the same time, very few studies
have explored the application of modern transformer-based models, such as Vision
Transformers (ViTs), in the domain of cytology. The global attention mechanism
in ViTs offers the potential to better capture structural features in cervical cells.
This thesis addresses these gaps by proposing a targeted two-stage approach:
• First, it trains separate WGAN-GP models for each cell class in the SIPaKMeD
dataset, allowing synthetic image generation to directly compensate for class
imbalance.
• Second, it fine-tunes a pretrained ViT-B/16 model on the augmented dataset
that includes both real and generated images, with the goal of improving
classification accuracy, especially for underrepresented cell types.
This direction builds upon the strengths of generative modeling and transformer
architectures to handle challenges of class imbalance, limited data, and
feature variability. Unlike prior works that treat all samples equally or rely on
standard augmentation, this study offers a more focused and data-driven solution
— specifically tuned to the characteristics of cervical cytology images.

SIPaKMeD Overview:
![image](https://github.com/user-attachments/assets/a0ee7847-d1d5-4509-8121-0c76a8af33c7)

Training the WGAN-GP Models:
To qualitatively evaluate the progress of training, sample outputs from the final
epoch were compiled across all five cell types. showcases representative
synthetic images generated by the respective WGAN-GP models.
![image](https://github.com/user-attachments/assets/a4a39f36-42b7-4e28-b98a-ed4ba045c238)

Results of ViT Classification:
  Experiment 1 (Real-Only):
    ![image](https://github.com/user-attachments/assets/5cdca805-aeef-4968-8b27-e2bf18ec6307)

  Experiment 2 (Synthetic-Only):
    ![image](https://github.com/user-attachments/assets/635b1449-40e7-4c82-a247-2d247c4829f9)
  Experiment 3 (Real + Synthetic):;
    ![image](https://github.com/user-attachments/assets/26c1f28b-2e85-44c1-9d04-2b97c74e597d)

Future Work
While the proposed pipeline achieved promising results, several directions remain
for future exploration:
• Inclusion of additional generative evaluation metrics such as Fréchet Inception
Distance (FID) and Structural Similarity Index (SSIM) for a more
complete analysis of synthetic image quality.
• Exploring advanced data augmentation strategies or domain adaptation methods
to further improve performance on minority classes.
• Investigating lightweight or hybrid transformer architectures for faster training
and deployment in real-time clinical applications.
• Testing the approach on other histopathology or cytology datasets to evaluate
its generalizability across medical imaging domains.
In conclusion, this research successfully demonstrated how modern generative and
transformer-based models can be integrated to address long-standing challenges in
medical image classification, setting a foundation for more reliable and balanced
AI-driven diagnostic tools.



